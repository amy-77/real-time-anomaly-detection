#!/usr/bin/env python3
"""
Weather Data Anomaly Detection System
-------------------------------------
A comprehensive system for detecting anomalies in weather station data using:
1. Temporal Analysis (ARIMA, STL, Statistical methods)
2. Spatial Verification (Neighbor trend correlation)
"""

import sqlite3
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import argparse
import json
import warnings
warnings.filterwarnings('ignore')


class WindowDataLoader:
    """Loads data within a sliding window from SQLite."""
    
    def __init__(self, db_path: str):
        self.conn = sqlite3.connect(db_path)
    
    def get_window_data(self, station_id: str, start_time: str = None, 
                       end_time: str = None, window_hours: int = None) -> pd.DataFrame:
        """
        è·å–æŒ‡å®šç«™ç‚¹çš„æ»‘åŠ¨çª—å£æ•°æ®
        æ”¯æŒä¸¤ç§æ¨¡å¼:
        1. æŒ‡å®š start_time + end_time: ä½¿ç”¨ç²¾ç¡®æ—¶é—´èŒƒå›´
        2. æŒ‡å®š end_time + window_hours: ä»end_timeå¾€å‰æ¨window_hourså°æ—¶
        """
        # æ¨¡å¼1: æŒ‡å®šäº†èµ·å§‹å’Œç»“æŸæ—¶é—´
        if start_time and end_time:
            start_dt = pd.to_datetime(start_time)
            end_dt = pd.to_datetime(end_time)
        
        # æ¨¡å¼2: æŒ‡å®šç»“æŸæ—¶é—´ + çª—å£é•¿åº¦
        elif end_time and window_hours:
            end_dt = pd.to_datetime(end_time)
            start_dt = end_dt - timedelta(hours=window_hours)
        
        else:
            raise ValueError("å¿…é¡»æŒ‡å®š: (start_time + end_time) æˆ– (end_time + window_hours)")
        
        # æŸ¥è¯¢æ•°æ®
        query = """
            SELECT time, temp_out, out_hum, wind_speed, bar, rain
            FROM observations
            WHERE station_id = ? AND time BETWEEN ? AND ?
            ORDER BY time ASC
        """
        df = pd.read_sql_query(query, self.conn, 
                               params=(station_id, start_dt.strftime('%Y-%m-%d %H:%M:%S'),
                                     end_dt.strftime('%Y-%m-%d %H:%M:%S')))
        df['time'] = pd.to_datetime(df['time'])
        return df
    
    def get_all_stations(self) -> pd.DataFrame:
        """è·å–æ‰€æœ‰ç«™ç‚¹ä¿¡æ¯"""
        return pd.read_sql_query("SELECT station_id, station_name_en, latitude, longitude, elevation FROM stations", self.conn)
    
    def close(self):
        if self.conn:
            self.conn.close()




class StatisticalDetector:
    """ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹å™¨"""
    
    @staticmethod
    def detect_3sigma(values: np.ndarray, threshold: float = 3.0) -> Tuple[np.ndarray, Dict]:
        """3Ïƒè§„åˆ™å¼‚å¸¸æ£€æµ‹"""
        if len(values) < 3:
            return np.zeros(len(values), dtype=bool), {}
        
        mean, std = np.mean(values), np.std(values)
        
        if std == 0:  # æ•°æ®æ— å˜åŒ–
            return np.zeros(len(values), dtype=bool), {
                'mean': mean, 'std': 0, 'upper_bound': mean, 
                'lower_bound': mean, 'is_constant': True
            }
        
        upper, lower = mean + threshold * std, mean - threshold * std
        anomalies = (values > upper) | (values < lower)
        
        return anomalies, {
            'mean': mean, 'std': std, 
            'upper_bound': upper, 'lower_bound': lower
        }
    
    @staticmethod
    def detect_iqr(values: np.ndarray, k: float = 1.5) -> Tuple[np.ndarray, Dict]:
        """IQRç®±çº¿å›¾æ³•"""
        if len(values) < 4:
            return np.zeros(len(values), dtype=bool), {}
        
        q1, q3 = np.percentile(values, [25, 75])
        iqr = q3 - q1
        
        if iqr == 0:
            median = np.median(values)
            return np.zeros(len(values), dtype=bool), {
                'q1': q1, 'q3': q3, 'iqr': 0, 'median': median, 'is_constant': True
            }
        
        lower = q1 - k * iqr
        upper = q3 + k * iqr
        anomalies = (values < lower) | (values > upper)
        
        return anomalies, {
            'q1': q1, 'q3': q3, 'iqr': iqr, 'median': np.median(values),
            'lower_bound': lower, 'upper_bound': upper
        }
    
    @staticmethod
    def detect_mad(values: np.ndarray, threshold: float = 3.5) -> Tuple[np.ndarray, Dict]:
        """MADä¸­ä½æ•°ç»å¯¹åå·®æ³•"""
        if len(values) < 3:
            return np.zeros(len(values), dtype=bool), {}
        
        median = np.median(values)
        mad = np.median(np.abs(values - median))
        
        if mad == 0:
            mad = np.mean(np.abs(values - median))
            if mad == 0:
                return np.zeros(len(values), dtype=bool), {
                    'median': median, 'mad': 0, 'is_constant': True
                }
        
        mad_scaled = 1.4826 * mad
        deviations = np.abs(values - median) / mad_scaled
        anomalies = deviations > threshold
        
        return anomalies, {
            'median': median, 'mad': mad, 'mad_scaled': mad_scaled,
            'threshold': threshold, 'std': mad_scaled
        }
    
    @staticmethod
    def detect_zscore(values: np.ndarray, threshold: float = 3.0) -> Tuple[np.ndarray, Dict]:
        """æ”¹è¿›çš„Z-scoreæ–¹æ³•ï¼ˆåŸºäºMADï¼‰"""
        if len(values) < 3:
            return np.zeros(len(values), dtype=bool), {}
        
        median = np.median(values)
        mad = np.median(np.abs(values - median))
        
        if mad == 0:
            return np.zeros(len(values), dtype=bool), {
                'median': median, 'mad': 0, 'is_constant': True
            }
        
        modified_z_scores = 0.6745 * (values - median) / mad
        anomalies = np.abs(modified_z_scores) > threshold
        
        return anomalies, {
            'median': median, 'mad': mad, 'threshold': threshold,
            'std': mad * 1.4826
        }
    
    @staticmethod
    def detect_percentile(values: np.ndarray, lower: float = 1, upper: float = 99) -> Tuple[np.ndarray, Dict]:
        """ç™¾åˆ†ä½æ•°æ–¹æ³•"""
        if len(values) < 10:
            return np.zeros(len(values), dtype=bool), {}
        
        lower_bound = np.percentile(values, lower)
        upper_bound = np.percentile(values, upper)
        anomalies = (values < lower_bound) | (values > upper_bound)
        
        return anomalies, {
            'lower_percentile': lower, 'upper_percentile': upper,
            'lower_bound': lower_bound, 'upper_bound': upper_bound,
            'median': np.median(values), 'std': np.std(values)
        }
    
    @staticmethod
    def detect_sudden_change(values: np.ndarray, max_change: float) -> np.ndarray:
        """æ£€æµ‹çªå˜ï¼ˆç›¸é‚»å€¼å˜åŒ–è¿‡å¤§ï¼‰"""
        if len(values) < 2:
            return np.zeros(len(values), dtype=bool)
        
        diffs = np.abs(np.diff(values))
        anomalies = np.zeros(len(values), dtype=bool)
        anomalies[1:] = diffs > max_change
        return anomalies


class TimeSeriesDetector:
    """æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹å™¨ - è€ƒè™‘æ—¶åºä¾èµ–æ€§"""
    
    @staticmethod
    def detect_arima_residuals(values: np.ndarray, threshold: float = 3.0) -> Tuple[np.ndarray, Dict]:
        """
        ARIMAæ®‹å·®åˆ†ææ³•
        åŸç†: ç”¨ARIMAé¢„æµ‹ï¼Œé¢„æµ‹è¯¯å·®è¿‡å¤§çš„ä¸ºå¼‚å¸¸
        ä¼˜åŠ¿: è€ƒè™‘æ—¶åºè‡ªç›¸å…³æ€§
        """
        try:
            from statsmodels.tsa.arima.model import ARIMA
        except ImportError:
            return np.zeros(len(values), dtype=bool), {'error': 'statsmodels not installed'}
        
        if len(values) < 20:
            return np.zeros(len(values), dtype=bool), {'error': 'insufficient data'}
        
        try:
            model = ARIMA(values, order=(1, 0, 1))
            fitted = model.fit()
            residuals = fitted.resid
            
            std_resid = np.std(residuals)
            if std_resid == 0:
                return np.zeros(len(values), dtype=bool), {}
            
            anomalies = np.abs(residuals) > threshold * std_resid
            
            return anomalies, {
                'mean_residual': float(np.mean(residuals)),
                'std_residual': float(std_resid),
                'aic': float(fitted.aic)
            }
        except Exception as e:
            return np.zeros(len(values), dtype=bool), {'error': str(e)}
    
    @staticmethod
    def detect_stl_residuals(values: np.ndarray, period: int = 6, 
                            threshold: float = 3.0) -> Tuple[np.ndarray, Dict]:
        """
        STLå­£èŠ‚-è¶‹åŠ¿åˆ†è§£
        åŸç†: åˆ†è§£ä¸ºè¶‹åŠ¿+å­£èŠ‚+æ®‹å·®ï¼Œæ®‹å·®å¼‚å¸¸ä¸ºçœŸå¼‚å¸¸
        ä¼˜åŠ¿: è‡ªåŠ¨å¤„ç†å‘¨æœŸæ€§å’Œè¶‹åŠ¿
        """
        try:
            from statsmodels.tsa.seasonal import STL
        except ImportError:
            return np.zeros(len(values), dtype=bool), {'error': 'statsmodels not installed'}
        
        if len(values) < 2 * period:
            return np.zeros(len(values), dtype=bool), {'error': f'need at least {2*period} points'}
        
        try:
            stl = STL(values, period=period, robust=True)
            result = stl.fit()
            residuals = result.resid
            
            median_resid = np.median(residuals)
            mad_resid = np.median(np.abs(residuals - median_resid))
            
            if mad_resid == 0:
                return np.zeros(len(values), dtype=bool), {}
            
            mad_scaled = 1.4826 * mad_resid
            anomalies = np.abs(residuals - median_resid) > threshold * mad_scaled
            
            return anomalies, {
                'median_residual': float(median_resid),
                'mad_residual': float(mad_resid)
            }
        except Exception as e:
            return np.zeros(len(values), dtype=bool), {'error': str(e)}


class MLDetector:
    """æœºå™¨å­¦ä¹ å¼‚å¸¸æ£€æµ‹å™¨"""
    
    @staticmethod
    def detect_isolation_forest(values: np.ndarray, contamination: float = 0.1) -> Tuple[np.ndarray, Dict]:
        """
        å­¤ç«‹æ£®æ—
        åŸç†: å¼‚å¸¸ç‚¹æ›´å®¹æ˜“è¢«å­¤ç«‹ï¼ˆåˆ’åˆ†ï¼‰
        ä¼˜åŠ¿: æ— éœ€å‡è®¾æ•°æ®åˆ†å¸ƒï¼Œé€‚åˆé«˜ç»´æ•°æ®
        """
        try:
            from sklearn.ensemble import IsolationForest
        except ImportError:
            return np.zeros(len(values), dtype=bool), {'error': 'sklearn not installed'}
        
        if len(values) < 10:
            return np.zeros(len(values), dtype=bool), {}
        
        X = values.reshape(-1, 1)
        clf = IsolationForest(contamination=contamination, random_state=42)
        predictions = clf.fit_predict(X)
        anomalies = predictions == -1
        
        return anomalies, {
            'contamination': contamination,
            'n_anomalies': int(np.sum(anomalies))
        }
    
    @staticmethod
    def detect_lof(values: np.ndarray, contamination: float = 0.1) -> Tuple[np.ndarray, Dict]:
        """
        å±€éƒ¨ç¦»ç¾¤å› å­ (LOF)
        åŸç†: åŸºäºå±€éƒ¨å¯†åº¦ï¼Œå¯†åº¦æ˜æ˜¾ä½äºé‚»å±…çš„ä¸ºå¼‚å¸¸
        ä¼˜åŠ¿: é€‚åˆå¯†åº¦ä¸å‡åŒ€çš„æ•°æ®
        """
        try:
            from sklearn.neighbors import LocalOutlierFactor
        except ImportError:
            return np.zeros(len(values), dtype=bool), {'error': 'sklearn not installed'}
        
        if len(values) < 10:
            return np.zeros(len(values), dtype=bool), {}
        
        X = values.reshape(-1, 1)
        clf = LocalOutlierFactor(contamination=contamination)
        predictions = clf.fit_predict(X)
        anomalies = predictions == -1
        
        return anomalies, {
            'contamination': contamination,
            'n_anomalies': int(np.sum(anomalies))
        }
    
    @staticmethod
    def detect_one_class_svm(values: np.ndarray, contamination: float = 0.1) -> Tuple[np.ndarray, Dict]:
        """
        One-Class SVM
        åŸç†: å­¦ä¹ "æ­£å¸¸æ•°æ®"çš„è¾¹ç•Œï¼Œè¶…å‡ºè¾¹ç•Œä¸ºå¼‚å¸¸
        ä¼˜åŠ¿: é€‚åˆå•ç±»åˆ«é—®é¢˜
        """
        try:
            from sklearn.svm import OneClassSVM
            from sklearn.preprocessing import StandardScaler
        except ImportError:
            return np.zeros(len(values), dtype=bool), {'error': 'sklearn not installed'}
        
        if len(values) < 10:
            return np.zeros(len(values), dtype=bool), {}
        
        X = values.reshape(-1, 1)
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        clf = OneClassSVM(nu=contamination, kernel='rbf', gamma='auto')
        predictions = clf.fit_predict(X_scaled)
        anomalies = predictions == -1
        
        return anomalies, {
            'contamination': contamination,
            'n_anomalies': int(np.sum(anomalies))
        }


class SpatialDetector:
    """ç©ºé—´å¼‚å¸¸æ£€æµ‹å™¨ - è€ƒè™‘åœ°ç†ä½ç½®ç›¸å…³æ€§"""
    
    @staticmethod
    def haversine_distance(lat1: float, lon1: float, lat2: float, lon2: float) -> float:
        """
        è®¡ç®—ä¸¤ç‚¹é—´çš„åœ°ç†è·ç¦»ï¼ˆå…¬é‡Œï¼‰
        """
        R = 6371  # åœ°çƒåŠå¾„ï¼ˆå…¬é‡Œï¼‰
        lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])
        dlat = lat2 - lat1
        dlon = lon2 - lon1
        a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
        c = 2 * np.arcsin(np.sqrt(a))
        return R * c
    
    
    @staticmethod
    def find_neighbors(station_idx: int, locations: np.ndarray, 
                      max_distance: float = 100,
                      max_elev_diff: float = 500) -> List[int]:
        """
        æ‰¾å‡ºæŸç«™ç‚¹çš„é‚»è¿‘ç«™ç‚¹ï¼ˆåŒæ—¶è€ƒè™‘æ°´å¹³è·ç¦»å’Œæµ·æ‹”å·®å¼‚ï¼‰
        å‚æ•°:
            station_idx: ç«™ç‚¹ç´¢å¼•
            locations: [[lat1, lon1, elev1], [lat2, lon2, elev2], ...]
            max_distance: æœ€å¤§æ°´å¹³è·ç¦»é˜ˆå€¼ï¼ˆå…¬é‡Œï¼‰
            max_elev_diff: æœ€å¤§æµ·æ‹”å·®å¼‚é˜ˆå€¼ï¼ˆç±³ï¼‰
        """
        neighbors = []
        target_lat, target_lon, target_elev = locations[station_idx]
        for i, loc in enumerate(locations):
            if i == station_idx:
                continue
            # 1. æ£€æŸ¥æµ·æ‹”å·®å¼‚
            elev_diff = abs(target_elev - loc[2])
            if elev_diff > max_elev_diff:
                continue
            # 2. æ£€æŸ¥æ°´å¹³è·ç¦»
            dist = SpatialDetector.haversine_distance(target_lat, target_lon, loc[0], loc[1])
            if dist <= max_distance:
                neighbors.append(i)
        return neighbors
    
    
    @staticmethod
    def elevation_adjusted_value(value: float, elev_diff: float, 
                                 var_type: str = 'temp') -> float:
        """
        æ ¹æ®æµ·æ‹”å·®å¼‚è°ƒæ•´å˜é‡å€¼
        æ°”è±¡å­¦ç»éªŒ:
        - æ¸©åº¦: æ¯å‡é«˜100mé™ä½0.65Â°C (å¹²ç»çƒ­é€’å‡ç‡)
        - æ°”å‹: æ¯å‡é«˜10mé™ä½çº¦1.2hPa
        - æ¹¿åº¦: æµ·æ‹”å½±å“è¾ƒå°ï¼Œä¸è°ƒæ•´
        """
        if var_type == 'temp':
            # æ¸©åº¦éšæµ·æ‹”é™ä½: -0.65Â°C/100m
            return value + (elev_diff / 100) * 0.65
        elif var_type == 'bar':
            # æ°”å‹éšæµ·æ‹”é™ä½: -1.2hPa/10m
            return value + (elev_diff / 10) * 1.2
        else:
            # å…¶ä»–å˜é‡ä¸è°ƒæ•´
            return value
    
    
    @staticmethod
    def detect_spatial_anomalies(
        station_data: Dict[str, Dict],  # {station_id: {var: value, lat, lon, elev}}
        variable: str,
        threshold: float = 3.0,
        max_distance: float = 100,
        min_neighbors: int = 2,
        max_elev_diff: float = 500
    ) -> Tuple[List[str], Dict]:
        """
        æ£€æµ‹ç©ºé—´å¼‚å¸¸ï¼Œæ ¸å¿ƒå¾ªç¯ï¼Œé€ä¸ªç«™ç‚¹å¯¹æ¯”ï¼Œç®—å‡ºåç¦»åº¦
        """
        station_ids = list(station_data.keys())
        n_stations = len(station_ids)
        if n_stations < min_neighbors + 1:
            return [], {'error': 'insufficient stations'}
        # æå–ä½ç½®å’Œå€¼
        locations = np.array([
            [station_data[sid]['latitude'], 
             station_data[sid]['longitude'],
             station_data[sid]['elevation']]
            for sid in station_ids
        ])
        values = np.array([station_data[sid].get(variable, np.nan) for sid in station_ids])
        # æ£€æµ‹æ¯ä¸ªç«™ç‚¹
        anomalous_stations = []
        details = {}
        for i, station_id in enumerate(station_ids):
            if np.isnan(values[i]):
                continue
            # æ‰¾é‚»è¿‘ç«™ç‚¹
            neighbor_indices = SpatialDetector.find_neighbors(i, locations, max_distance, max_elev_diff)
            if len(neighbor_indices) < min_neighbors:
                continue  # é‚»å±…å¤ªå°‘ï¼Œæ— æ³•åˆ¤æ–­
            
            # è·å–é‚»è¿‘ç«™ç‚¹çš„å€¼ï¼ˆè€ƒè™‘æµ·æ‹”ä¿®æ­£ï¼‰
            target_elev = locations[i, 2]
            neighbor_values_adjusted = []
            neighbor_raw_values = []
            
            for j in neighbor_indices:
                if np.isnan(values[j]):
                    continue
                
                elev_diff = locations[j, 2] - target_elev
                adjusted_val = SpatialDetector.elevation_adjusted_value(
                    values[j], elev_diff, var_type=variable
                )
                neighbor_values_adjusted.append(adjusted_val)
                neighbor_raw_values.append(values[j])
            
            if len(neighbor_values_adjusted) < min_neighbors:
                continue
            
            # è®¡ç®—é‚»è¿‘ç«™ç‚¹çš„ç»Ÿè®¡é‡
            neighbor_median = np.median(neighbor_values_adjusted)
            neighbor_mad = np.median(np.abs(np.array(neighbor_values_adjusted) - neighbor_median))
            
            if neighbor_mad == 0:
                neighbor_mad = np.std(neighbor_values_adjusted) or 1e-6
            
            # è®¡ç®—åç¦»ç¨‹åº¦
            deviation = abs(values[i] - neighbor_median) / (1.4826 * neighbor_mad)
            
            if deviation > threshold:
                anomalous_stations.append(station_id)
                details[station_id] = {
                    'value': float(values[i]),
                    'neighbor_median': float(neighbor_median),
                    'neighbor_mad': float(neighbor_mad),
                    'deviation': float(deviation),
                    'n_neighbors': len(neighbor_values_adjusted),
                    'neighbor_ids': [station_ids[j] for j in neighbor_indices],
                    'neighbor_raw_values': [float(x) for x in neighbor_raw_values],
                    'neighbor_adj_values': [float(x) for x in neighbor_values_adjusted]
                }
                
        return anomalous_stations, details





class AnomalyDetector:
    """å¼‚å¸¸æ£€æµ‹ä¸»æ§åˆ¶å™¨"""
    
    # æ”¯æŒçš„æ£€æµ‹æ–¹æ³•
    AVAILABLE_METHODS = {
        # ç»Ÿè®¡æ–¹æ³•ï¼ˆåŸºäºåˆ†å¸ƒï¼‰
        '3sigma': '3Ïƒè§„åˆ™ - å‡è®¾æ­£æ€åˆ†å¸ƒ',
        'iqr': 'IQRç®±çº¿å›¾æ³• - é²æ£’ï¼Œé€‚åˆåæ€æ•°æ®', # ä¸å¯ä¿¡ï¼Œå¼ƒç”¨
        'mad': 'MADä¸­ä½æ•°ç»å¯¹åå·® - æŠ—å™ªå£°',  # ä¸å¯ä¿¡ï¼Œå¼ƒç”¨
        'zscore': 'æ”¹è¿›Z-score - åŸºäºMAD',
        'percentile': 'ç™¾åˆ†ä½æ•°æ³• - å®šä¹‰ç¨€æœ‰åº¦',
        
        # æ—¶åºæ–¹æ³•ï¼ˆè€ƒè™‘æ—¶é—´ä¾èµ–ï¼‰
        'arima': 'ARIMAæ®‹å·®æ³• - è€ƒè™‘è‡ªç›¸å…³æ€§',
        'stl': 'STLåˆ†è§£æ³• - å¤„ç†è¶‹åŠ¿å’Œå‘¨æœŸ',
        
        # æœºå™¨å­¦ä¹ æ–¹æ³•
        'isolation_forest': 'å­¤ç«‹æ£®æ— - æ— åˆ†å¸ƒå‡è®¾',
        'lof': 'å±€éƒ¨ç¦»ç¾¤å› å­ - åŸºäºå¯†åº¦',
        'ocsvm': 'One-Class SVM - å­¦ä¹ æ­£å¸¸è¾¹ç•Œ',
        
        # ç©ºé—´æ–¹æ³•ï¼ˆè€ƒè™‘åœ°ç†ä½ç½®ï¼‰
        'spatial': 'ç©ºé—´å¼‚å¸¸æ£€æµ‹ - åŸºäºé‚»è¿‘ç«™ç‚¹ç›¸å…³æ€§'
    }
    
    # æ£€æµ‹å˜é‡é…ç½®
    DETECTION_VARS = {
        'temp_out': {'name': 'æ¸©åº¦', 'unit': 'Â°C', 'threshold': 3, 'sudden_change': 5.0},
        'out_hum': {'name': 'æ¹¿åº¦', 'unit': '%', 'threshold': 3},
        'wind_speed': {'name': 'é£é€Ÿ', 'unit': 'km/h', 'threshold': 3},
        'bar': {'name': 'æ°”å‹', 'unit': 'hPa', 'threshold': 3, 'sudden_change': 10.0}
    }
    
    def __init__(self, db_path: str = 'weather_stream.db', 
                 start_time: str = None, end_time: str = None, window_hours: int = None,
                 temporal_method: str = '3sigma', spatial_method: str = 'mad', spatial_verify: bool = False):
        """
        Initialize Anomaly Detector.
        
        Args:
            db_path: Path to SQLite database
            start_time: Window start time (YYYY-MM-DD HH:MM:SS)
            end_time: Window end time (YYYY-MM-DD HH:MM:SS)
            window_hours: Window duration in hours
            temporal_method: Method for temporal detection (e.g., 'arima', '3sigma')
            spatial_method: Method for spatial fallback (e.g., 'mad')
            spatial_verify: Enable spatial cross-verification (Trend Analysis)
        """
        self.start_time = start_time
        self.end_time = end_time
        self.window_hours = window_hours
        self.temporal_method = temporal_method
        self.spatial_method = spatial_method
        self.spatial_verify = spatial_verify
        
        # Validate parameters
        if not ((start_time and end_time) or (end_time and window_hours)):
            raise ValueError("Must specify: (start_time + end_time) OR (end_time + window_hours)")
        
        # Validate temporal method
        if temporal_method not in self.AVAILABLE_METHODS:
            raise ValueError(f"Unsupported method: {temporal_method}. Available: {list(self.AVAILABLE_METHODS.keys())}")
        
        self.loader = WindowDataLoader(db_path)
        self.stat_detector = StatisticalDetector()
        self.ts_detector = TimeSeriesDetector()
        self.ml_detector = MLDetector()
    
    
    
    def verify_spatial_trend(self, station_id: str, timestamp: str, 
                           variable: str, window_minutes: int = 30) -> Dict:
        """
        é«˜çº§ç©ºé—´éªŒè¯ï¼šåŸºäºè¶‹åŠ¿ç›¸å…³æ€§ (Pearson Correlation)
        é€»è¾‘:
            1. å–ç›®æ ‡ç«™ç‚¹å’Œé‚»å±…åœ¨ [T-window, T+window] çš„æ•°æ®
            2. å¯¹æ•°æ®è¿›è¡Œçº¿æ€§æ’å€¼å¡«è¡¥ç©ºæ´
            3. è®¡ç®—ç›®æ ‡ç«™ç‚¹ä¸é‚»å±…çš„çš®å°”é€Šç›¸å…³ç³»æ•°
            4. å¦‚æœå¼ºç›¸å…³(>0.7) -> ç¯å¢ƒå˜åŒ–; å¼±ç›¸å…³(<0.3) -> è®¾å¤‡æ•…éšœ
        """
        # 1. ç¡®å®šæ—¶é—´èŒƒå›´
        dt = pd.to_datetime(timestamp)
        start_dt = dt - timedelta(minutes=window_minutes)
        end_dt = dt + timedelta(minutes=window_minutes)
        
        # 2. è·å–æ‰€æœ‰ç«™ç‚¹ä½ç½®ä¿¡æ¯ï¼ˆä¸ºäº†æ‰¾é‚»å±…ï¼‰
        stations_df = self.loader.get_all_stations()
        locations = np.array([
            [row['latitude'], row['longitude'], row['elevation']]
            for _, row in stations_df.iterrows()
        ])
        station_ids = stations_df['station_id'].tolist()
        
        try:
            target_idx = station_ids.index(station_id)
        except ValueError:
            return {'error': 'station not found'}
            
        # 3. æ‰¾å‡ºé‚»å±… (ä½¿ç”¨ SpatialDetector çš„é™æ€æ–¹æ³•)
        neighbor_indices = SpatialDetector.find_neighbors(
            target_idx, locations, max_distance=100, max_elev_diff=500
        )
        
        if not neighbor_indices:
            return {'status': 'no_neighbors', 'correlation': 0, 'msg': 'æ— é‚»å±…'}
            
        neighbor_ids = [station_ids[i] for i in neighbor_indices]
        
        # 4. æŸ¥è¯¢æ•°æ® (ç›®æ ‡ç«™ç‚¹ + é‚»å±…)
        # æ„é€ æŸ¥è¯¢æ‰€æœ‰æ¶‰åŠç«™ç‚¹çš„æ•°æ®
        all_ids = [station_id] + neighbor_ids
        placeholders = ','.join(['?'] * len(all_ids))
        
        query = f"""
            SELECT time, station_id, {variable}
            FROM observations
            WHERE station_id IN ({placeholders})
            AND time BETWEEN ? AND ?
            ORDER BY time
        """
        
        params = all_ids + [start_dt.strftime('%Y-%m-%d %H:%M:%S'), 
                          end_dt.strftime('%Y-%m-%d %H:%M:%S')]
        
        df = pd.read_sql_query(query, self.loader.conn, params=params)
        
        if df.empty:
            return {'status': 'no_data', 'correlation': 0}
            
        # 5. æ•°æ®é€è§†ä¸æ¸…æ´—
        df['time'] = pd.to_datetime(df['time'])
        pivot_df = df.pivot(index='time', columns='station_id', values=variable)
        
        # ç¡®ä¿ç›®æ ‡ç«™ç‚¹åœ¨åˆ—ä¸­
        if station_id not in pivot_df.columns:
            return {'status': 'no_data', 'correlation': 0}
            
        # 6. æ’å€¼å¡«è¡¥ç©ºæ´ (å…³é”®æ­¥éª¤)
        # ä½¿ç”¨æ—¶é—´ç´¢å¼•è¿›è¡Œçº¿æ€§æ’å€¼ï¼Œé™åˆ¶æ’å€¼æ–¹å‘
        pivot_df = pivot_df.interpolate(method='time', limit_direction='both', limit=2)
        
        # å†æ¬¡æ¸…ç†ä»ä¸ºNaNçš„è¡Œï¼ˆæ’å€¼å¤±è´¥çš„ï¼‰
        pivot_df.dropna(inplace=True)
        
        if len(pivot_df) < 5:  # æ•°æ®ç‚¹å¤ªå°‘æ— æ³•è®¡ç®—ç›¸å…³æ€§
            return {'status': 'insufficient_points', 'correlation': 0}
            
        # 7. è®¡ç®—ç›¸å…³ç³»æ•°
        target_series = pivot_df[station_id]
        correlations = []
        valid_neighbors = []
        
        for nid in neighbor_ids:
            if nid in pivot_df.columns:
                # è®¡ç®—çš®å°”é€Šç³»æ•°
                corr = target_series.corr(pivot_df[nid])
                if not np.isnan(corr):
                    correlations.append(corr)
                    valid_neighbors.append(nid)
        
        if not correlations:
            return {'status': 'no_valid_correlations', 'correlation': 0}
            
        # å–ç›¸å…³ç³»æ•°çš„ä¸­ä½æ•°æˆ–æœ€å¤§å€¼ä½œä¸ºæœ€ç»ˆåˆ¤å®šä¾æ®
        # è¿™é‡Œå–ä¸­ä½æ•°æ¯”è¾ƒç¨³å¥
        median_corr = np.median(correlations)
        max_corr = np.max(correlations)
        
        return {
            'status': 'success',
            'median_corr': median_corr,
            'max_corr': max_corr,
            'n_neighbors': len(correlations),
            'valid_neighbors': valid_neighbors,
            'is_trend_consistent': median_corr > 0.6 or max_corr > 0.8
        }




    def detect_station(self, station_id: str) -> Dict:
        """Detect anomalies for a single station."""
        # Load data
        df = self.loader.get_window_data(station_id, 
                                         start_time=self.start_time,
                                         end_time=self.end_time, 
                                         window_hours=self.window_hours)
        
        # Validate data
        if df.empty:
            return {'station_id': station_id, 'status': 'no_data', 'message': 'No Data'}
        if len(df) < 3:
            return {'station_id': station_id, 'status': 'insufficient_data', 
                   'message': f'Insufficient Data ({len(df)} points)'}
        
        # Initialize result structure
        result = {
            'station_id': station_id,
            'window_start': str(df['time'].min()),
            'window_end': str(df['time'].max()),
            'data_count': len(df),
            'anomalies': {},
            'has_anomaly': False
        }
        
        # Detect each variable
        for var, config in self.DETECTION_VARS.items():
            anomaly_info = self._detect_variable(df, var, config)
            if anomaly_info:
                # Spatial Verification (if enabled)
                if self.spatial_verify:
                    for record in anomaly_info['anomaly_records']:
                        # === Advanced: Spatial Trend Verification ===
                        # Use same window length as temporal detection, look backwards only
                        trend_res = self.verify_spatial_trend(
                            station_id=station_id,
                            timestamp=record['time'],
                            variable=var,
                            window_minutes=self.window_hours * 60
                        )
                        
                        if trend_res.get('status') == 'success':
                            corr = trend_res['median_corr']
                            if trend_res['is_trend_consistent']:
                                record['type'] = 'weather_event'
                                record['label'] = 'ğŸŒ§ï¸ Extreme Weather / Env Change'
                                record['desc'] = f"Trend Consistent (Corr: {corr:.2f}, {trend_res['n_neighbors']} neighbors)"
                            elif corr < 0.3:
                                record['type'] = 'critical_failure'
                                record['label'] = 'ğŸ”´ Device Failure (High Confidence)'
                                record['desc'] = f"Trend Inconsistent (Corr: {corr:.2f}, erratic)"
                            else:
                                # Grey area (0.3 ~ 0.6)
                                record['type'] = 'warning'
                                record['label'] = 'âš ï¸ Suspected Anomaly (Manual Check)'
                                record['desc'] = f"Weak Correlation (Corr: {corr:.2f})"
                        else:
                            # Fallback to static snapshot comparison if trend verification fails
                            spatial_res = self.detect_spatial_anomalies(
                                timestamp=record['time'], 
                                variable_filter=var, 
                                method=self.spatial_method,
                                verbose=False
                            )
                            
                            # Check if it's an anomaly in snapshot
                            if var in spatial_res['variables'] and \
                               station_id in spatial_res['variables'][var]['anomalous_stations']:
                                record['label'] = 'ğŸ”´ Device Failure (Static Check)'
                                record['desc'] = f"High Deviation (Trend Skipped: {trend_res.get('status', 'unknown')})"
                            else:
                                record['label'] = 'ğŸŒ§ï¸ Extreme Weather (Static Check)'
                                record['desc'] = f"Low Deviation (Trend Skipped: {trend_res.get('status', 'unknown')})"
                
                # --- DEBUG: Print full sequence for context ---
                print(f"\n{'='*40} DEBUG: Sequence Data {'='*40}")
                print(f"Station: {station_id}, Variable: {var}")
                print(f"Window: {df['time'].min()} ~ {df['time'].max()}")
                print("-" * 100)
                
                # Extract time series
                times = df['time'].dt.strftime('%H:%M').values
                vals = df[var].values
                
                # Print
                for t, v in zip(times, vals):
                    mark = ""
                    # Check if this point is flagged
                    for rec in anomaly_info['anomaly_records']:
                        if rec['time'].endswith(t + ":00"):
                            mark = f"<--- âš ï¸  Anomaly ({rec.get('deviation', 0):.1f}Ïƒ)"
                            break
                    print(f"{t} | {v:8.2f} {config['unit']} {mark}")
                print(f"{'='*100}\n")
                # --------------------------------------------

                result['anomalies'][var] = anomaly_info
                result['has_anomaly'] = True
        
        return result
    
    def _detect_variable(self, df: pd.DataFrame, var: str, config: Dict) -> Optional[Dict]:
        """æ£€æµ‹å•ä¸ªå˜é‡çš„å¼‚å¸¸"""
        if var not in df.columns:
            return None
        
        values = df[var].values
        if np.all(np.isnan(values)):
            return None
        
        # æ ¹æ®æ–¹æ³•é€‰æ‹©æ£€æµ‹å™¨
        if self.temporal_method == '3sigma':
            anomaly_mask, stats = self.stat_detector.detect_3sigma(values, config['threshold'])
        elif self.temporal_method == 'iqr':
            anomaly_mask, stats = self.stat_detector.detect_iqr(values, k=1.5)
        elif self.temporal_method == 'mad':
            anomaly_mask, stats = self.stat_detector.detect_mad(values, threshold=3.5)
        elif self.temporal_method == 'zscore':
            anomaly_mask, stats = self.stat_detector.detect_zscore(values, threshold=3.0)
        elif self.temporal_method == 'percentile':
            anomaly_mask, stats = self.stat_detector.detect_percentile(values, lower=1, upper=99)
        elif self.temporal_method == 'arima':
            anomaly_mask, stats = self.ts_detector.detect_arima_residuals(values, threshold=3.0)
        elif self.temporal_method == 'stl':
            anomaly_mask, stats = self.ts_detector.detect_stl_residuals(values, period=6, threshold=3.0)
        elif self.temporal_method == 'isolation_forest':
            anomaly_mask, stats = self.ml_detector.detect_isolation_forest(values, contamination=0.1)
        elif self.temporal_method == 'lof':
            anomaly_mask, stats = self.ml_detector.detect_lof(values, contamination=0.1)
        elif self.temporal_method == 'ocsvm':
            anomaly_mask, stats = self.ml_detector.detect_one_class_svm(values, contamination=0.1)
        else:
            # é»˜è®¤3sigma
            anomaly_mask, stats = self.stat_detector.detect_3sigma(values, config['threshold'])
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
        if 'error' in stats:
            return None
        
        # çªå˜æ£€æµ‹ï¼ˆä½œä¸ºé¢å¤–æ£€æµ‹ï¼Œå¦‚æœé…ç½®äº†ï¼‰
        if 'sudden_change' in config:
            sudden_mask = self.stat_detector.detect_sudden_change(values, config['sudden_change'])
            anomaly_mask = anomaly_mask | sudden_mask
        
        # å¦‚æœæ²¡æœ‰å¼‚å¸¸ï¼Œè¿”å›None
        if not np.any(anomaly_mask):
            return None
        
        # æ„å»ºå¼‚å¸¸è®°å½•
        anomaly_indices = np.where(anomaly_mask)[0]
        anomaly_records = []
        
        for idx in anomaly_indices:
            record = {
                'time': str(df.iloc[idx]['time']),
                'value': float(values[idx])
            }
            
            # è®¡ç®—åç¦»åº¦ï¼ˆæ ¹æ®æ–¹æ³•ï¼‰
            if self.temporal_method in ['3sigma', 'zscore', 'arima', 'stl'] and stats.get('std', 0) > 0:
                mean_val = stats.get('mean', stats.get('median', 0))
                record['deviation'] = float(abs(values[idx] - mean_val) / stats['std'])
            elif self.temporal_method == 'iqr' and stats.get('iqr', 0) > 0:
                record['deviation'] = float(abs(values[idx] - stats['median']) / stats['iqr'])
            elif self.temporal_method == 'mad' and stats.get('mad_scaled', 0) > 0:
                record['deviation'] = float(abs(values[idx] - stats['median']) / stats['mad_scaled'])
            else:
                record['deviation'] = 0.0
            
            anomaly_records.append(record)
        
        return {
            'name': config['name'],
            'unit': config['unit'],
            'count': int(np.sum(anomaly_mask)),
            'method': self.temporal_method,
            'statistics': {k: float(v) for k, v in stats.items() if k not in ['is_constant', 'error']},
            'anomaly_records': anomaly_records
        }
    
    def detect_all_stations(self) -> List[Dict]:
        """æ£€æµ‹æ‰€æœ‰ç«™ç‚¹"""
        stations_df = self.loader.get_all_stations()
        results = []
        
        for _, row in stations_df.iterrows():
            result = self.detect_station(row['station_id'])
            result['station_name'] = row['station_name_en']
            results.append(result)
        
        return results
    
    def detect_spatial_anomalies(self, timestamp: str = None, 
                                max_distance: float = 100,
                                threshold: float = 3.0,
                                max_elev_diff: float = 500,
                                variable_filter: str = None,
                                method: str = 'mad',
                                verbose: bool = True) -> Dict:
        """
        ç©ºé—´å¼‚å¸¸æ£€æµ‹
        å‚æ•°:
            method: ç©ºé—´æ£€æµ‹æ–¹æ³• (æ”¯æŒ 'mad', 'zscore', '3sigma')
        """
        # è·å–æ‰€æœ‰ç«™ç‚¹ä¿¡æ¯
        stations_df = self.loader.get_all_stations()
        # è·å–æŒ‡å®šæ—¶åˆ»çš„æ•°æ®
        if timestamp is None:
            # ä½¿ç”¨çª—å£ç»“æŸæ—¶åˆ»
            timestamp = self.end_time
        # æŸ¥è¯¢æ‰€æœ‰ç«™ç‚¹åœ¨è¯¥æ—¶åˆ»çš„æ•°æ®
        query = """
            SELECT o.station_id, o.temp_out, o.out_hum, o.wind_speed, o.bar,
                   s.latitude, s.longitude, s.elevation, s.station_name_en
            FROM observations o
            JOIN stations s ON o.station_id = s.station_id
            WHERE o.time = ?
        """
        df = pd.read_sql_query(query, self.loader.conn, params=(timestamp,))
        if df.empty:
            return {'error': 'no data at specified time', 'timestamp': timestamp}
        
        # å‡†å¤‡ç«™ç‚¹æ•°æ®å­—å…¸
        station_data = {}
        for _, row in df.iterrows():
            station_data[row['station_id']] = {
                'latitude': row['latitude'],
                'longitude': row['longitude'],
                'elevation': row['elevation'],
                'temp_out': row['temp_out'],
                'out_hum': row['out_hum'],
                'wind_speed': row['wind_speed'],
                'bar': row['bar'],
                'station_name': row['station_name_en']
            }
        
        # å¯¹æ¯ä¸ªå˜é‡è¿›è¡Œç©ºé—´æ£€æµ‹
        results = {
            'timestamp': timestamp,
            'n_stations': len(station_data),
            'max_distance': max_distance,
            'max_elev_diff': max_elev_diff,
            'threshold': threshold,
            'variables': {}
        }
        
        spatial_detector = SpatialDetector()
        
        # --- æ‰“å°ä¸€æ¬¡é‚»å±…å…³ç³» (ä»…åŸºäºåœ°ç†ä½ç½®å’Œæµ·æ‹”ï¼Œä¸ä¾èµ–å˜é‡) ---
        station_ids = list(station_data.keys())
        n_stations = len(station_ids)
        
        if n_stations > 1 and verbose:
            locations = np.array([
                [station_data[sid]['latitude'], 
                 station_data[sid]['longitude'],
                 station_data[sid]['elevation']]
                for sid in station_ids
            ])
            
            print(f"\n{'='*80}")
            print(f"Spatial Neighbor Analysis (Max Dist: {max_distance}km, Max Elev Diff: {max_elev_diff}m)")
            print(f"{'='*80}")
            
            for i, station_id in enumerate(station_ids):
                neighbor_indices = spatial_detector.find_neighbors(i, locations, max_distance, max_elev_diff)
                
                if not neighbor_indices:
                    print(f"Station {station_id}: No neighbors found")
                    continue
                
                print(f"Station {station_id} ({station_data[station_id]['station_name']}) - {len(neighbor_indices)} neighbors:")
                target_elev = locations[i, 2]
                
                for idx in neighbor_indices:
                    nid = station_ids[idx]
                    n_name = station_data[nid]['station_name']
                    n_elev = locations[idx, 2]
                    dist = spatial_detector.haversine_distance(
                        locations[i, 0], locations[i, 1], 
                        locations[idx, 0], locations[idx, 1]
                    )
                    elev_diff = n_elev - target_elev
                    print(f"  -> {nid} ({n_name}): Dist {dist:.1f}km, Elev {n_elev:.0f}m (Diff: {elev_diff:+.0f}m)")
            print(f"{'='*80}\n")

        # --- æ‰§è¡Œå˜é‡æ£€æµ‹ ---
        for var, config in self.DETECTION_VARS.items():
            # å¦‚æœæŒ‡å®šäº†å˜é‡è¿‡æ»¤å™¨ï¼Œè·³è¿‡ä¸ç›¸å…³çš„å˜é‡
            if variable_filter and var != variable_filter:
                continue
                
            anomalous_stations, details = spatial_detector.detect_spatial_anomalies(
                station_data, var, threshold, max_distance, min_neighbors=2,
                max_elev_diff=max_elev_diff
            )
            
            if anomalous_stations:
                results['variables'][var] = {
                    'name': config['name'],
                    'unit': config['unit'],
                    'anomalous_stations': anomalous_stations,
                    'details': details
                }
        
        return results
    
    def close(self):
        self.loader.close()





class ReportGenerator:
    """Generates analysis reports."""
    @staticmethod
    def generate_text_report(results: List[Dict], window_info: str = None, method: str = '3sigma') -> str:
        """Generate text report."""
        method_names = {
            '3sigma': '3-Sigma Rule', 'iqr': 'IQR Method', 'mad': 'MAD (Median Absolute Deviation)',
            'zscore': 'Modified Z-Score', 'percentile': 'Percentile',
            'arima': 'ARIMA Residuals', 'stl': 'STL Decomposition',
            'isolation_forest': 'Isolation Forest', 'lof': 'Local Outlier Factor', 'ocsvm': 'One-Class SVM'
        }
        
        lines = [
            "ANOMALY DETECTION REPORT",
            f"Detection Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"Window Info: {window_info}" if window_info else "",
            f"Method: {method_names.get(method, method)}",
            ""
        ]
        
        # Statistics
        total = len(results)
        stations_with_anomaly = [r for r in results if r.get('has_anomaly', False)]
        anomaly_count = len(stations_with_anomaly)
        
        # Breakdown by anomaly type
        type_counts = {'critical_failure': 0, 'weather_event': 0, 'warning': 0}
        
        for r in stations_with_anomaly:
            for var_info in r['anomalies'].values():
                for rec in var_info['anomaly_records']:
                    atype = rec.get('type', 'unknown')
                    if atype in type_counts:
                        type_counts[atype] += 1
        
        lines.extend([
            f"Total Stations: {total}",
            f"Anomalous Stations: {anomaly_count}",
            f"Normal Stations: {total - anomaly_count}",
            "",
            "Anomaly Breakdown:",
            f"  ğŸ”´ Device Failures: {type_counts['critical_failure']}",
            f"  ğŸŒ§ï¸ Weather Events: {type_counts['weather_event']} (Ignorable)",
            f"  âš ï¸ Suspected:      {type_counts['warning']}",
            "",
            "=" * 100,
            ""
        ])
        
        # Detailed Info
        for result in results:
            if result.get('has_anomaly'):
                lines.extend(ReportGenerator._format_station_anomalies(result))
        
        return "\n".join(lines)
    
    @staticmethod
    def _format_station_anomalies(result: Dict) -> List[str]:
        """Format anomalies for a single station."""
        lines = [
            f"[Station: {result['station_id']} - {result.get('station_name', 'Unknown')}]",
            f"  Window: {result['window_start']} ~ {result['window_end']}",
            f"  Data Points: {result['data_count']}",
            ""
        ]
        
        for var, info in result['anomalies'].items():
            stats = info['statistics']
            lines.append(f"  âš ï¸  {info['name']} Anomaly:")
            lines.append(f"      Count: {info['count']}")
            lines.append(f"      Method: {info.get('method', 'unknown')}")
            
            # Stats
            if 'mean' in stats and 'std' in stats:
                lines.append(f"      Stats: Mean={stats['mean']:.2f}{info['unit']}, Std={stats['std']:.2f}{info['unit']}")
            elif 'median' in stats and 'std' in stats:
                lines.append(f"      Stats: Median={stats['median']:.2f}{info['unit']}, Std={stats['std']:.2f}{info['unit']}")
            elif 'median' in stats and 'iqr' in stats:
                lines.append(f"      Stats: Median={stats['median']:.2f}{info['unit']}, IQR={stats['iqr']:.2f}{info['unit']}")
            
            # Normal Range
            if 'lower_bound' in stats and 'upper_bound' in stats:
                lines.append(f"      Normal Range: [{stats['lower_bound']:.2f}, {stats['upper_bound']:.2f}] {info['unit']}")
            
            # Records
            for record in info['anomaly_records']:
                line = f"      â€¢ {record['time']}: {record['value']:.2f}{info['unit']} (Dev: {record['deviation']:.1f}Ïƒ)"
                if 'label' in record:
                    line += f" -> {record['label']}"
                lines.append(line)
                if 'desc' in record:
                     lines.append(f"        â””â”€ Diag: {record['desc']}")
            lines.append("")
        
        return lines
    
    @staticmethod
    def save_json_report(results: List[Dict], window_info: dict = None, filename: str = None) -> str:
        """Save JSON report."""
        if filename is None:
            filename = f"anomaly_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        report_data = {
            'detection_time': datetime.now().isoformat(),
            'results': results
        }
        
        if window_info:
            report_data['window_info'] = window_info
        
        with open(filename, 'w') as f:
            json.dump(report_data, f, indent=2)
        
        return filename





def main():
    parser = argparse.ArgumentParser(
        description='Weather Data Anomaly Detection System',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Statistical Method
  python anomaly_detector.py --end "2025-11-20 16:00:00" --window 6 --temporal-method 3sigma
  
  # Time-Series Method (Recommended)
  python anomaly_detector.py --end "2025-11-20 16:00:00" --window 6 --temporal-method arima --spatial-verify
  
  # Machine Learning Method
  python anomaly_detector.py --end "2025-11-20 16:00:00" --window 6 --temporal-method lof
  
  # List all methods
  python anomaly_detector.py --list-methods
        """
    )
    
    parser.add_argument('--db', default='weather_stream.db', help='Path to SQLite DB')
    parser.add_argument('--start', help='Start time (YYYY-MM-DD HH:MM:SS)')
    parser.add_argument('--end', help='End time (YYYY-MM-DD HH:MM:SS)')
    parser.add_argument('--window', type=int, help='Window size (hours)')
    parser.add_argument('--temporal-method', default='3sigma',
                       choices=list(AnomalyDetector.AVAILABLE_METHODS.keys()),
                       help='Temporal method (default: 3sigma)')
    parser.add_argument('--spatial-method', default='mad',
                       choices=['mad', 'zscore', '3sigma'],
                       help='Spatial fallback method (default: mad)')
    parser.add_argument('--station', help='Target station ID')
    parser.add_argument('--save', action='store_true', help='Save result to JSON')
    parser.add_argument('--spatial-verify', action='store_true', 
                       help='Enable spatial trend verification')
    parser.add_argument('--quiet', action='store_true', help='Quiet mode')
    parser.add_argument('--list-methods', action='store_true', help='List all methods')
    
    args = parser.parse_args()
    
    # åˆ—å‡ºæ£€æµ‹æ–¹æ³•
    if args.list_methods:
        print("\n" + "="*80)
        print("Available Detection Methods:")
        print("="*80)
        for method, desc in AnomalyDetector.AVAILABLE_METHODS.items():
            print(f"  {method:20s} - {desc}")
        print("="*80 + "\n")
        return
    
    # Validate args
    if not ((args.start and args.end) or (args.end and args.window)):
        parser.error("Must specify: (--start AND --end) OR (--end AND --window)")
    
    # Prepare window info
    if args.start and args.end:
        window_info_str = f"{args.start} ~ {args.end}"
        window_info_dict = {'start_time': args.start, 'end_time': args.end}
    else:
        window_info_str = f"End at {args.end} (Lookback {args.window} hours)"
        window_info_dict = {'end_time': args.end, 'window_hours': args.window}
    
    window_info_dict['temporal_method'] = args.temporal_method
    
    # æ‰§è¡Œæ£€æµ‹
    detector = AnomalyDetector(
        db_path=args.db, 
        start_time=args.start,
        end_time=args.end,
        window_hours=args.window,
        temporal_method=args.temporal_method,
        spatial_method=args.spatial_method,
        spatial_verify=args.spatial_verify
    )
    
    try:
        # æ£€æµ‹
        if args.station:
            results = [detector.detect_station(args.station)]
        else:
            results = detector.detect_all_stations()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = ReportGenerator.generate_text_report(results, window_info_str, method=args.temporal_method)
        
        if not args.quiet:
            print(report)
        
        # ä¿å­˜JSON
        if args.save:
            filename = ReportGenerator.save_json_report(results, window_info_dict)
            print(f"\n æ£€æµ‹ç»“æœå·²ä¿å­˜åˆ°: {filename}")
    
    finally:
        detector.close()


if __name__ == '__main__':
    main()


## ç”¨ç»Ÿè®¡æ–¹æ³• - arimaï¼Œisolation_forestï¼Œ3sigma
# python anomaly_detector.py --end "2025-11-22 17:00:00" --window 6 --temporal-method arima --spatial-verify
# python anomaly_detector.py --end "2025-11-22 17:00:00" --window 6 --temporal-method isolation_forest --spatial-verify